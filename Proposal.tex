\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}



\title{EE227BT Project:
Understanding the Continuous-Time Limit of Nesterov Acceleration}
\author{Nilesh Tripuraneni (nilesh\_tripuraneni@berkeley.edu, 3032089919)\\ Sarah Dean (sarahdean@eecs.berkeley.edu, 3031893242)\\ Jeffrey Chan (chanjed@berkeley.edu, 24988067)\\ Aldo Pacchiano (pacchiano@berkeley.edu, 26995108)}
\date{October 25, 2016}
\begin{document}
\maketitle


\section{Summary}

Our project will focus on understanding recent approaches to analyzing the continuous-time limit of ``accelerated'' gradient descent algorithms for soliving optimization problems \cite{su2014differential, wibisono2016variational, krichene2015accelerated}. We will do a literature review and then experiment with deriving different discrete-time algorithms of  the continuous-time formulations using different numerical discretization schemes.

\section{Background}
% Gradient descent update
First order methods for solving convex optimization problems of the form
\[\min f(x)\]
are popular for many machine-learning tasks and large-scale problems. Perhaps the earliest method, gradient descent, dates back to Euler and Lagrange:
\[x_{k+1} = x_k - s \nabla f(x_k)\]
where the discrete-time system is initialized at $x_0$. In a seminal paper, Nesterov proposed an ``accelerated" gradient method \cite{nesterov1983method}, which introduces an auxiliary ``momentum" variable:
\begin{align*}
    x_k &= y_{k-1} - s \nabla f(y_{k-1})\\
    y_k &= x_k + \frac{k-1}{k+2} (x_k - x_{k-1}),
\end{align*}
initialized at $x_0$ and $y_0 = x_0$. Importantly, 
for any fixed step size $s \leq \frac{1}{L}$ where $L$ is the Lipschitz constant of $\nabla f$, the Nesterov accelerated gradient scheme has a convergence rate of:
 
\begin{equation*}
f(x_k) - f^* \leq O \left(\frac{\|x_0 - x^*\|^2}{sk^2} \right)
\end{equation*}
which achieves the optimal convergence rate amongst first order methods \footnote{The first-order oracle complexity lower bound for first order methods applies to black-box optimization problems which use the previous trajectory $(x_1, g_1, \cdots, x_t, g_t)$ where $g_i \in \partial f(x_i)$ to determine the next point $x_{t+1}$ \cite{blair1985problem, nesterov2004introductory}.}. Indeed, the while the convergence rate of the accelerated gradient scheme is quadratic, the ordinary gradient descent scheme only has a linear convergence rate.

This accelerated method has notable applications in sparse linear regression\cite{beck2009fast}\cite{qin2012structured}, compressed sensing \cite{becker2011nesta}, and deep and recurrent neural networks \cite{sutskever2013importance}. Recently, Su et al. derived a second order ODE which is the continuous time limit of Nesterov's method \cite{su2014differential}. The ODE takes the form
\[ \ddot{X} + \frac{3}{t} \dot{X} + \nabla f(X) = 0 \]
By analyzing properties of this ODE, a generalized family of accelerated methods can be constructed and many of the peculiar properties observed in Nesterov's method can be more deeply understood. 

Other recent work has extended this analysis to a more general, non-Euclidean setting in mirror decent and using the Bregman divergence \cite{wibisono2016variational, krichene2015accelerated}. 

\section{Proposed Work}
We propose to investigate the interface between the discrete-time Nesterov acceleration algorithm and its underlying continuous-time ODE that has been recently derived. The first stage of our project will be a literature review. We will start by reviewing \cite{su2014differential, wibisono2016variational, krichene2015accelerated} in addition to \cite{DBLP:journals/ftml/Bubeck15}. 

Understanding the continuous-time limit of the Nesterov acceleration procedure offers a simple and intuitive perspective (and proofs) of many of the underlying phenomena associated with the algorithm (such as the celebrated $\mathcal{O}(\frac{1}{k^2})$ convergence rate). Additionally, this view has motivated several new algorithms such as a restarting algorithm, which re-initializes the descent algorithm when it becomes too "slow" \cite{su2014differential}. 
One issue of practical importance is to understand the subtleties of the discretization procedure of the continuous-time ODE that preserve the accelerated convergence rate. Following the literature review, the second half of our project will reproduce the existing experiments which have attempted to discretize both the accelerated and accelerated mirror-descent ODEs. After these initial experiments, we will numerically study the impact and efficiency of more sophisticated discretization schemes (such as higher-order Runge-Kutta schemes and symplectic integrators) which are known to have good stability properties \cite{hairer2006geometric}. 


\bibliographystyle{unsrt}
\bibliography{refs}





\end{document}
