In this project we have given a review of results in the intersection of the study of continuous time ODEs with first order methods for convex optimization procedures. We review the basic results in gradient descent that establish convergence and give lower bounds. We then review a well known acceleration method, and examine it from the continuous time perspective. We establish the equivalence of the continuous time results.

We then extend fron the Lyapunov perspective in continuous time to recover Accelerated Mirror descent, a family of accelerated methods for the non-euclidean domain. We note the specifics necessary when performing the discretization procedure, and show the equivalence with continuous time. 

\subsection{Variational framework in continuous time}
The framework of using a continuous time ODE and drawing relationships to these methods can fully generalize and unite the various attempts. 

Summarize major unifying perspectives from this paper

\subsubsection{Insights from continuous time}
Mention the restart method, see the  

\subsubsection{downsides of continuous time}
mention examples where the continuous time perspective loses all the interesting information

\subsection{Discretization}
Throughout the analysis, it becomes clear that there are particularities having to do with the way these continuous time functions are decreased. Vastly different algorithms in discrete time can have the same continuous time limit. Need for principles in discretization procedure (ashia's newest paper).

\subsection{Conclusion}
paragraph summary of the paper and the work that we did followed by paragraph summary of the major insights into the field as a whole