
\subsection{Gradient descent and its limitations}

Gradient descent is an iterative optimization algorithm that can be traced back to Cauchy in 1847 that can be used to minimise a convex function $f$ over $\mathbb{R}$. The iterations evolve according to the following update equation:

\begin{equation}
    x_{t+1} = x_t - \eta \nabla f(x_t)
\end{equation}

Where $\eta > 0$ is called the step size. 

One can study the convergence properties of Gradient descent for convex functions provided some restrictions are imposed on the class of functions subject to study. We focus on the study of $\beta-$smooth and $\alpha-$strongly convex functions. 

\subsection{Smoothness}

\begin{definition}

We say that a continous differentiable function is $\beta$-smooth if the gradient $\nabla f$ is $\beta-$Lipschitz, that is:

\begin{equation}
    \parallel \nabla f(x) - \nabla f(y) \parallel \leq \beta \parallel x - y \parallel
\end{equation}

\end{definition}

This is equivalent to say that the eigenvalues of the Hessian being smaller than $\beta$. 
The convergence rate for gradient descent convergence for $\beta-$smooth functions is:

\begin{theorem}

Let $f$ be convex and $\beta$-smooth on $\mathbb{R}^n$, then gradient descent with $\eta = \frac{1}{\beta}$ satisfies:

\begin{equation}
f(x_t) - f(x^*) \leq \frac{2\beta \parallel x_1 - x^* \parallel^2}{t-1}
\end{equation}

\end{theorem}


Note:

The proof hinges on the following equivalent characterization of $\beta-$smooth functions:

\begin{equation}\label{fundamental_beta_eq}
0 \leq f(x) - f(y) - \nabla f(y)^T(x-y) \leq \frac{\beta}{2} \parallel x - y \parallel^2
\end{equation}

A $\beta$-smooth function is dominated by a quadratic. 

In particular Equation \ref{fundamental_beta_eq} immediately implies a bound for the minimal improvement of a single gradient descent step.

\begin{equation}
f(x- \frac{1}{\beta} \nabla f(x) ) - f(x) \leq -\frac{1}{2\beta} \parallel \nabla f(x) \parallel^2
\end{equation}










\subsection{Strong convexity}

\begin{definition}
We say that $f: \mathcal{X} \rightarrow \mathbb{R}$ is $\alpha-$ strongly convex if it satisfies the following inequality:

\begin{equation}
f(x) - f(y) \leq \nabla f(x)^T (x-y) - \frac{\alpha}{2} \parallel x -y \parallel^2
\end{equation}

\end{definition}

Alternatively, a function $f$ is strongly convex if $x \rightarrow f(x) - \frac{\alpha}{2}\parallel x \parallel^2$ is convex. In the case of twice differentiable functions it is equivalent to saying that the eigenvalues of the Hessian be bounded below by $\alpha$. 

In the case of strongly convex functions one can prove the following convergence upper bound for gradient descent:

\begin{theorem}
If $f$ is $\alpha-$strongly convex and and $L-$lipschitz ($|f(x) - f(y)| \leq L \parallel x - y\parallel $) on a domain $\mathcal{X}$, gradient descent (taking a gradient step and proyecting it back to the domain $\mathcal{X}$) with dynamic step size $\eta_s = \frac{1}{\alpha(s+1)}$ at time $s$ satisfies the following bound:

\begin{equation}
f\left(\sum_{s=1}^t \frac{2s}{t(t+1)} x_s \right) - f(x^*) \leq \frac{2L^2}{\alpha (t+1)}
\end{equation}

\end{theorem}









\subsubsection{Strongly convex and smooth functions}

Assume $f$ to be $\alpha-$ strongly convex and $\beta-$ smooth. 

\begin{definition}
The \textbf{condition number} of $f$ is $\kappa = \frac{\beta}{\alpha}$. Then:
\end{definition}

We can show the following convergence rate for functions that are both $\alpha-$strongly convex and $\beta-$smooth:

\begin{theorem}
If $f$ is $\alpha-$strongly convex and $\beta-$smooth over $\mathcal{X}$ then projected gradient descent with $\eta = \frac{1}{\beta}$ satisfies for $t \geq 0$:

\begin{equation}
\parallel x_{t+1} - x^*\parallel^2 \leq \exp(-\frac{t}{\kappa})\parallel x_1 - x^*\parallel^2
\end{equation}


\end{theorem}

We can also derive a result showing exponential convergence on the value of the function. 

\begin{theorem}
Let $f$ be $\beta-$smooth and $\alpha-$strongly convex on $\mathbb{R}^n$. Then gradient descent with $\eta = \frac{2}{\alpha+\beta}$ satisfies:
\begin{equation}
f(x_{t+1}) - f(x^*) \leq \frac{\beta}{2} \exp( -\frac{4t}{\kappa + 1}) \parallel x_1 - x^* \parallel^2
\end{equation}
\end{theorem}



\subsubsection{Lower bounds}

A black box procedure is a mapping from the history of the algorithm to the next query point. In other words, it maps $(x_1, g_1, \cdots, x_t, g_t)$ such that $g_t \in \partial f(x_t)$ to $x_{t+1}$. And such that $x_1 = 0$ and $x_{t+1}$ is in the linear span of $g_1, \cdots, g_t$. 

Denote $e_1, \cdots, e_n$ the canonical basis of $\mathbb{R}^n$, and $B_2(R) = \{ x \in \mathbb{R}^n : \parallel x \parallel \leq R\}$. 

In this section we present lower bounds for the convergence rate of black box optimization procedures for different classes of convex functions. 

We first show that for $\beta-$smooth convex functions there is no black box procedure achieving a faster convergence rate than $\frac{1}{t^2}$. 



\begin{theorem}
Let $t \leq (n-1)/2 , \beta >0$, then there is a $\beta-$smooth convex function such that any black box procedure for which $x_{t+1} \in Span(g_1, \cdots, g_t)$: 

\begin{equation}
\min_{1 \leq s \leq t } f(x_s ) - f(x^*) \geq \frac{3\beta}{32} \frac{\parallel x_1 - x^*\parallel}{(t+1)^2}
\end{equation}

\end{theorem}




\proofstart


Consider the following quadratic function:

\begin{equation}
    f(x) = \frac{\beta}{8}x^T A_{2t+1}x- \frac{\beta}{4}x^Te_1
\end{equation}


Where $A_k$ is an $n\times n$ matrix defined as:

\begin{equation}
(A_k)_{i,j} = \begin{cases}
                2  &  i = i,j \leq k \\
                -1 &  j \in \{i-1, i+1\}, i \leq k, j \neq k+1\\
                0 & \text{o.w.}
            \end{cases}
\end{equation}

 
Define $f_k(x) = \frac{\beta}{8} x^T A_k x - \frac{\beta}{4}x^Te_1$ and for any function $g$ define $g^* = \min_{x \in \mathbb{R}} g(x)$.

The matrix $A_k$ satisfies the following additional properties:
\begin{itemize}
\item[1] $0 \preceq A_k \preceq 4I_n$
\item[2] $x^TA_kx = x(1)^2 + x(k)^2 + \sum_{i=1}^{k-1} (x(i)-x(i+1))^2$.
\item[3] If $y(i) = 0$ for $i \geq r$, then $y^T A_{k} y = y^TA_{r}y$ and therefore by 2) $\nabla f(y) \in span(e_1, \cdots, e_{r})$. 
\item[4] The minimiser $x_k^*$ of $f_k(x)$ and its optimal value $f_k^*$ satisfy:
    \begin{align}
        x_k^*(i) = \begin{cases}
                    1-\frac{i}{k+1} & i = 1, \cdots, k \\
                    0 & \text{o.w.}
                    \end{cases}\\
        f_k^* = -\frac{\beta}{8}\left( 1-\frac{1}{k+1}\right)
    \end{align}
\end{itemize}


Notice that $x_s$ is in the span of $e_1, \cdots, e_{s-1}$.  Recall that $x_1 = 0$, and therefore that $\partial(f(x_1)) = -\frac{\beta}{4}e_1$, meaning that $x_2 \in span(e_1)$, by an inductive application of 3) we conclude that $x_s \in span(e_1, \cdots, e_{s-1})$. 

Combining the previous observations we conclude the following string of inequalities:

\begin{equation}
f(x_s) - f^* = f_s(x_s) - f_{2t+1}^* \geq f_s^* - f^*_{2t+1} \geq f_t^* - f_{2t+1}^*
\end{equation}

Since $\parallel x_k^* \parallel^2 = \sum_{i=1}^k \left( \frac{i}{k+1}\right)^2 \leq \frac{k+1}{3}$ we obtain:

\begin{equation}
f_t^* - f_{2t+1}^* =\frac{\beta}{8}\left( \frac{1}{t+1} - \frac{1}{2t+2}\right) \geq \frac{3\beta}{32} \frac{\parallel x_{2t+1}*\parallel^2}{(t+1)^2}
\end{equation}

This concludes the proof. 

\proofend


\begin{theorem}
If the condition number $\kappa > 1$, then there is a $\beta-$smooth and $\alpha-$strongly convex function $f: l_2 \rightarrow \mathbb{R}$ with $\kappa = \frac{\beta}{\alpha}$ such that for any $t \geq 1$ and any black box procedure for which $x_{k+1} \in Span( g_1, \cdots, g_t)$:

\begin{equation}
f(x_t) - f(x^*) \geq \frac{\alpha}{2} \left(\frac{ \sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{2(t-1)} \parallel x_1 - x^* \parallel^2
\end{equation}

When $\kappa$ is large, $\left(\frac{ \sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{2(t-1)} \parallel x_1 - x^* \parallel^2 \approx \exp( -\frac{4(t-1)}{\sqrt{\kappa}})$


\end{theorem}

\proofstart

The main ideas of this proof are very similar to those used in the previous one. Denote by $A$ the infinite dimensional linear operator corresponding to an inifinite matrix with $2$ in the diagonal, and $-1$ on the upper and lower diagonals. This operator is a generalization of the finite dimensional $A_k$ operators found in the previous section. 

We will show that the following $\alpha-$convex and $\beta-$smooth function:

\begin{equation}
f(x) = \frac{\alpha(\kappa-1)}{8} \left( \langle Ax, x \rangle - 2\langle e_1, x \rangle \right) + \frac{\alpha}{2} \parallel x \parallel^2
\end{equation}


Since $f$ is $\alpha-$strongly convex $f(x_t) - f(x^*) \geq \frac{\alpha}{2}\parallel x_t - x^*\parallel^2$. Therefore it only remains to lower bound $\parallel x_t - x^*\parallel^2$.

$A$ has similar properties to its finite version, namely $0 \preceq A \preceq 4I$ and $x_t(i) =0 \forall i \geq t$. The later immediately implies that: 

\begin{equation}
\parallel x_t - x^* \parallel^2 \geq \sum_{i=t}^\infty x^*(i)^2
\end{equation}

To instantiate the lower bound we compute $x^*$. After differentiating and setting the gradient to zero we find the optimum is achieved by $x^*$ satisfying:

\begin{equation}
x^*(i) = \left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} +1} \right)^i 
\end{equation}

Putting all these bound together:

\begin{equation}
f(x_t) - f(x^*) \geq \frac{\alpha}{2}\sum_{i=t}^\infty \left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} +1} \right)^{2i} 
\end{equation}

Recall that $x_1 =0$. The geometric sum can be rewritten as 
\begin{equation}
\left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} +1} \right)^{2(t-1)} \left( \sum_{i=1}^\infty \left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} +1} \right)^{2i} \right) = \left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} +1} \right)^{2(t-1)} \parallel x_1 - x^* \parallel^2
\end{equation}

This concludes the proof.

\proofend



